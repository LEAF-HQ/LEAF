{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LEAF: An LHC Event Analysis Framework Welcome to the documentation for the LHC event analysis framework (LEAF)! If you have not used LEAF before, it is advisable to read the intoduction below, it will give an overview over the different purposes LEAF can be used for. Otherwise, you might as well just skip it and refer to the documentation of the individual packages for in-depth information. tl;dr: Install LEAF and follow the quickstart example Introduction LEAF is a versatile, relatively lightweight framework that allows to perform multiple steps common to about every LHC data analysis. It has several \"packages\", each of which may be used for a different task analysts might encounter on their way from a theoretical model to a full publication. LEAF may currently only be used for CMS data analysis, as it is specific to the data format adopted in the CMS Collaboration. Additionally, it is only configured for use with the PSI Tier-3 SLURM cluster, but it is possible to extend it to different infrastructures. Instructions on how to install and complie LEAF can be found on the Installation page. Brief introductions to all packages are given below on this page, in-depth information can be found in the pages linked in the respective section. The typical workflow from zero to a full physics result usually follows the order in which packages are introduced here. A quickstart example of an analysis is available here. Generator The Generator package is used in the very beginning of an analysis to generate signal samples from a theoretical model in the UFO format using the MadGraph5_aMCatNLO event generator. A UFO model is often provided by authors of phenomenological papers. With the Generator, it is possible to privately generate events of whatever process the model is able to simulate and to process them down the chain of all data formats employed by CMS. Tuplizer The Tuplizer packages picks up where the Generator left. It is used to convert samples from a CMS event format to the format used by LEAF. This means that it is possible to process both samples generated privately and those produced centrally. It can access files stored anywhere in the LHC computing grid (LCG), be it local, Tier-3, or Tier-2 storage elements. Depending on the need of the users new versions of tuples can be generated in certain intervals. Using the database integrated in the Generator package, it is possible to use a common set of tuples for all users, such that it is not necessary for everyone to generate their own tuples. Analyzer The Analyzer package arguably is the most heavily used LEAF package. It reads events in the LEAF format generated by the Tuplizer and offers all functionalities to perform a full-fledged data analysis. Event selection and the generation of histograms in a concise and efficient way are the key features of this package. It is interfaced to the Tuplizer in order to ensure a consistent data format. However, every user is given the possibility to extend it according to their needs, making the Analyzer a very versatile tool able to accomodate various flavors of analyses. Submitter The Submitter package is a helper tool for the Analyzer . It allows for convenient yet efficient submission of jobs to the local cluster. Plotter The Plotter is the last package in the LEAF chain. Having been processed by the Analyzer and Submitter packages, all histograms stored in a file according to the LEAF format can be visualized by the Plotter. It offers customizability and a coherent style of plots as well as the option to save plots in a collated way or individually.","title":"Home"},{"location":"#leaf-an-lhc-event-analysis-framework","text":"Welcome to the documentation for the LHC event analysis framework (LEAF)! If you have not used LEAF before, it is advisable to read the intoduction below, it will give an overview over the different purposes LEAF can be used for. Otherwise, you might as well just skip it and refer to the documentation of the individual packages for in-depth information. tl;dr: Install LEAF and follow the quickstart example","title":"LEAF: An LHC Event Analysis Framework"},{"location":"#introduction","text":"LEAF is a versatile, relatively lightweight framework that allows to perform multiple steps common to about every LHC data analysis. It has several \"packages\", each of which may be used for a different task analysts might encounter on their way from a theoretical model to a full publication. LEAF may currently only be used for CMS data analysis, as it is specific to the data format adopted in the CMS Collaboration. Additionally, it is only configured for use with the PSI Tier-3 SLURM cluster, but it is possible to extend it to different infrastructures. Instructions on how to install and complie LEAF can be found on the Installation page. Brief introductions to all packages are given below on this page, in-depth information can be found in the pages linked in the respective section. The typical workflow from zero to a full physics result usually follows the order in which packages are introduced here. A quickstart example of an analysis is available here.","title":"Introduction"},{"location":"#generator","text":"The Generator package is used in the very beginning of an analysis to generate signal samples from a theoretical model in the UFO format using the MadGraph5_aMCatNLO event generator. A UFO model is often provided by authors of phenomenological papers. With the Generator, it is possible to privately generate events of whatever process the model is able to simulate and to process them down the chain of all data formats employed by CMS.","title":"Generator"},{"location":"#tuplizer","text":"The Tuplizer packages picks up where the Generator left. It is used to convert samples from a CMS event format to the format used by LEAF. This means that it is possible to process both samples generated privately and those produced centrally. It can access files stored anywhere in the LHC computing grid (LCG), be it local, Tier-3, or Tier-2 storage elements. Depending on the need of the users new versions of tuples can be generated in certain intervals. Using the database integrated in the Generator package, it is possible to use a common set of tuples for all users, such that it is not necessary for everyone to generate their own tuples.","title":"Tuplizer"},{"location":"#analyzer","text":"The Analyzer package arguably is the most heavily used LEAF package. It reads events in the LEAF format generated by the Tuplizer and offers all functionalities to perform a full-fledged data analysis. Event selection and the generation of histograms in a concise and efficient way are the key features of this package. It is interfaced to the Tuplizer in order to ensure a consistent data format. However, every user is given the possibility to extend it according to their needs, making the Analyzer a very versatile tool able to accomodate various flavors of analyses.","title":"Analyzer"},{"location":"#submitter","text":"The Submitter package is a helper tool for the Analyzer . It allows for convenient yet efficient submission of jobs to the local cluster.","title":"Submitter"},{"location":"#plotter","text":"The Plotter is the last package in the LEAF chain. Having been processed by the Analyzer and Submitter packages, all histograms stored in a file according to the LEAF format can be visualized by the Plotter. It offers customizability and a coherent style of plots as well as the option to save plots in a collated way or individually.","title":"Plotter"},{"location":"installation/","text":"Installation Getting the code (once) Getting the LEAF code is easy, there is a script that takes care of everything for you. It will set up LEAF and a CMSSW version in the directory the script is executed from. You will need access to /cvmfs in order for the compilation to work. The latest tag that has been tested extensively is used. After downloading the code and checking out the necessary repositories, most notably the JECDatabase , JRDatabase , and Combine , some packages of LEAF will be compiled to generate all executables used in the various steps of a data analysis. This is all done automatically, so nothing to worry about. The following snippet will install LEAF and several versions of CMSSW in the folder the snippet below is executed from. It will reuse existing installations of CMSSW of the correct versions, if there are any. Make sure to run in a clean shell: wget https://raw.githubusercontent.com/LEAF-HQ/LEAF/v1.0/get_leaf.sh source get_leaf.sh Note You should now close the current shell and continue in a fresh one. At this point, LEAF is fully functional and all its packages can be used. You should now close this shell and start again in a fresh one. If it is intended to perform a usual data analysis, it is necessary to set up a new analysis skeleton, which can be modified and extended according to each user's needs and wishes. Simply follow the remaining steps on this page. Setup (every time) Each package of LEAF can be used more or less standalone, even though there are some interconnections between the packages. To set up LEAF in a fresh shell, simply cd into your LEAF folder and source the setup script: cd <PATH/TO/LEAF> source setup.sh Setting up a new analysis (once) There is a handy macro that creates all structures needed for a new analysis and integrates it into the LEAF compilation workflow. All that needs to be done (in an environment where LEAF has already been set up ) is cd $LEAFPATH ./createNewAnalysis.py <NewAnalysisName> cd Analyzer make clean make -j This will create all necessary files and folders for the new analysis to be compiled and linked automatically when comiling the Analyzer package. See the Analyzer documentation for details. It is advisable to choose a concise name for <NewAnalysisName> , it will be typed frequently. Also make sure the first few letters are convenient to type. If you are new to LEAF, you can follow the quickstart guide to run a small, self-contained test analysis, introducing some essential features. If you already know what you are doing, you are all set. Detailed information can be found in the documentation of each LEAF package, which is linked from the top navigation bar and from the homepage .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#getting-the-code-once","text":"Getting the LEAF code is easy, there is a script that takes care of everything for you. It will set up LEAF and a CMSSW version in the directory the script is executed from. You will need access to /cvmfs in order for the compilation to work. The latest tag that has been tested extensively is used. After downloading the code and checking out the necessary repositories, most notably the JECDatabase , JRDatabase , and Combine , some packages of LEAF will be compiled to generate all executables used in the various steps of a data analysis. This is all done automatically, so nothing to worry about. The following snippet will install LEAF and several versions of CMSSW in the folder the snippet below is executed from. It will reuse existing installations of CMSSW of the correct versions, if there are any. Make sure to run in a clean shell: wget https://raw.githubusercontent.com/LEAF-HQ/LEAF/v1.0/get_leaf.sh source get_leaf.sh Note You should now close the current shell and continue in a fresh one. At this point, LEAF is fully functional and all its packages can be used. You should now close this shell and start again in a fresh one. If it is intended to perform a usual data analysis, it is necessary to set up a new analysis skeleton, which can be modified and extended according to each user's needs and wishes. Simply follow the remaining steps on this page.","title":"Getting the code (once)"},{"location":"installation/#setup-every-time","text":"Each package of LEAF can be used more or less standalone, even though there are some interconnections between the packages. To set up LEAF in a fresh shell, simply cd into your LEAF folder and source the setup script: cd <PATH/TO/LEAF> source setup.sh","title":"Setup (every time)"},{"location":"installation/#setting-up-a-new-analysis-once","text":"There is a handy macro that creates all structures needed for a new analysis and integrates it into the LEAF compilation workflow. All that needs to be done (in an environment where LEAF has already been set up ) is cd $LEAFPATH ./createNewAnalysis.py <NewAnalysisName> cd Analyzer make clean make -j This will create all necessary files and folders for the new analysis to be compiled and linked automatically when comiling the Analyzer package. See the Analyzer documentation for details. It is advisable to choose a concise name for <NewAnalysisName> , it will be typed frequently. Also make sure the first few letters are convenient to type. If you are new to LEAF, you can follow the quickstart guide to run a small, self-contained test analysis, introducing some essential features. If you already know what you are doing, you are all set. Detailed information can be found in the documentation of each LEAF package, which is linked from the top navigation bar and from the homepage .","title":"Setting up a new analysis (once)"},{"location":"quickstart/","text":"Quickstart guide Welcome to the quickstart guide for performing an analysis using LEAF! The steps below will introduce some essential features of any data analysis. Prerequisites This guide assumes you have just installed LEAF and created a fresh analysis following the instructions in the installation guide . Note For this guide, we will assume the new analysis was named MyAnalysis . If you chose a different name, please adjust all the commands below accordingly. You will not need to modify any code, but only the commands to be executed If you have not yet set up your own analysis, either complete the installation and setup of a new analysis first, or make sure to adjust the commands and file contents shown below to your structure. Event selection and histogram production The first step will be to loop over some test samples of (mock-)data, a simulated background, and two simulated signals. Within the loop, events will be selected and a number of histograms will be filled. We will use the AnalysisTool class to perform this task. A new incarnation of such a tool was created when setting up the new analysis. It can be found in LEAF/MyAnalysis/Analyzer/src/MyAnalysisTool.cc . The settings of the job, such as the files to run over or the input/output paths are configured in an xml file, which can be found in LEAF/MyAnalysis/Analyzer/config/MyAnalysis.xml . Be sure to have a look into both files and understand what is going on. The configuration file is already prepared to run over the test samples (in LEAF/.testsamples/*.root ), so we can simply run the Analyzer executable locally, passing the configuration file as an argument: cd $LEAFPATH/MyAnalysis/Analyzer/config Analyzer MyAnalysis.xml Note The above command can be run from anywhere, then the relative (or absolute, if preferred) path to the xml file must be adjusted accordingly. However, this is not true for all programs along the way, so it is best to execute all commands from within the folder that contains the file passed as an argument. The default configuration file will instruct the Analyzer to store the output root files in a folder on the same level as the main LEAF folder, MyAnalysis/2017/Preselection/InitialSetup . To get there, do cd $LEAFPATH/../MyAnalysis/2017/Preselection/InitialSetup You will find four root files in that folder, one for each of the samples we processed. In a real analysis, a given process will often be split into different samples binned in a certain variable (for example HT, PT, or M). In order to merge samples adding up to the same process, we will use the Submitter : cd $LEAFPATH/MyAnalysis/Analyzer/config submit.py MyAnalysis.xml -p Note In this exercise, this step would not really be necessary, but in a full-fledged analysis it will be. You can see two new files created, one for data and one for background. Note that the names of the new samples are slightly changed. Plotting distributions The next step will be to visualize the events processed by plotting distributions of relevant observables and comparing different processes. The Plotter package takes care of this. Just like the Analyzer executable, also the Plotter executable is configured through an xml file, which contains information on the datasets to be plotted. When creating the new analysis, a corresponding configuration file for the Plotter has already been created, so simply plot the distributions: cd $LEAFPATH/MyAnalysis/Plotter Plotter Default.xml The distributions will be created in the folder containing the output root files of the previous step, which is on the same level as the main LEAF folder: MyAnalysis/2017/Preselection/InitialSetup/plots/distributions . To get there, do cd $LEAFPATH/../MyAnalysis/2017/Preselection/InitialSetup/plots/distributions Feel free to inspect the plots before moving on. Also feel free to try and add some new event selection steps and plot the default distributions after those. Adding more observables could be useful as well. Post-analysis After having selected events and plotted distributions at the different stages of the event selection, now we will perform some further analysis to understand our selection and finally infer the presence or absence of a signal over the standard model expectation. Together with the new analysis came a basic collection of analysis scripts, with are contained in the PostAnalyzer folder. It contains one main executable script. Let us execute it: cd $LEAFPATH/MyAnalysis/Analyzer/PostAnalyzer ./steer.py This will produce two results at the moment. They are new plots to be found in the usual output folder. To get there, do cd $LEAFPATH/../MyAnalysis/2017/Preselection/InitialSetup/plots The first are two plots of the efficiency of each selection step applied in the beginning of this quickstart, one with a linear and one with a logarithmic y-axis scale. This type of plot is useful for monitoring the behavior of the event selections applied. The second new result is the expected upper limit on our test signal cross section, as extracted from the selected background and signal. Note that the data are not considered here for the moment. Conclusion In this short example, you have performed a pre-defined event selection, you have visualized the intermediate steps and the final outcome. Last, you have run additional scripts on your fully selected events to compute the selection efficiency of each step and expected upper limits on the signal production cross section. Details on each LEAF package can be found in this documentation as well, see the links in here .","title":"Quickstart"},{"location":"quickstart/#quickstart-guide","text":"Welcome to the quickstart guide for performing an analysis using LEAF! The steps below will introduce some essential features of any data analysis.","title":"Quickstart guide"},{"location":"quickstart/#prerequisites","text":"This guide assumes you have just installed LEAF and created a fresh analysis following the instructions in the installation guide . Note For this guide, we will assume the new analysis was named MyAnalysis . If you chose a different name, please adjust all the commands below accordingly. You will not need to modify any code, but only the commands to be executed If you have not yet set up your own analysis, either complete the installation and setup of a new analysis first, or make sure to adjust the commands and file contents shown below to your structure.","title":"Prerequisites"},{"location":"quickstart/#event-selection-and-histogram-production","text":"The first step will be to loop over some test samples of (mock-)data, a simulated background, and two simulated signals. Within the loop, events will be selected and a number of histograms will be filled. We will use the AnalysisTool class to perform this task. A new incarnation of such a tool was created when setting up the new analysis. It can be found in LEAF/MyAnalysis/Analyzer/src/MyAnalysisTool.cc . The settings of the job, such as the files to run over or the input/output paths are configured in an xml file, which can be found in LEAF/MyAnalysis/Analyzer/config/MyAnalysis.xml . Be sure to have a look into both files and understand what is going on. The configuration file is already prepared to run over the test samples (in LEAF/.testsamples/*.root ), so we can simply run the Analyzer executable locally, passing the configuration file as an argument: cd $LEAFPATH/MyAnalysis/Analyzer/config Analyzer MyAnalysis.xml Note The above command can be run from anywhere, then the relative (or absolute, if preferred) path to the xml file must be adjusted accordingly. However, this is not true for all programs along the way, so it is best to execute all commands from within the folder that contains the file passed as an argument. The default configuration file will instruct the Analyzer to store the output root files in a folder on the same level as the main LEAF folder, MyAnalysis/2017/Preselection/InitialSetup . To get there, do cd $LEAFPATH/../MyAnalysis/2017/Preselection/InitialSetup You will find four root files in that folder, one for each of the samples we processed. In a real analysis, a given process will often be split into different samples binned in a certain variable (for example HT, PT, or M). In order to merge samples adding up to the same process, we will use the Submitter : cd $LEAFPATH/MyAnalysis/Analyzer/config submit.py MyAnalysis.xml -p Note In this exercise, this step would not really be necessary, but in a full-fledged analysis it will be. You can see two new files created, one for data and one for background. Note that the names of the new samples are slightly changed.","title":"Event selection and histogram production"},{"location":"quickstart/#plotting-distributions","text":"The next step will be to visualize the events processed by plotting distributions of relevant observables and comparing different processes. The Plotter package takes care of this. Just like the Analyzer executable, also the Plotter executable is configured through an xml file, which contains information on the datasets to be plotted. When creating the new analysis, a corresponding configuration file for the Plotter has already been created, so simply plot the distributions: cd $LEAFPATH/MyAnalysis/Plotter Plotter Default.xml The distributions will be created in the folder containing the output root files of the previous step, which is on the same level as the main LEAF folder: MyAnalysis/2017/Preselection/InitialSetup/plots/distributions . To get there, do cd $LEAFPATH/../MyAnalysis/2017/Preselection/InitialSetup/plots/distributions Feel free to inspect the plots before moving on. Also feel free to try and add some new event selection steps and plot the default distributions after those. Adding more observables could be useful as well.","title":"Plotting distributions"},{"location":"quickstart/#post-analysis","text":"After having selected events and plotted distributions at the different stages of the event selection, now we will perform some further analysis to understand our selection and finally infer the presence or absence of a signal over the standard model expectation. Together with the new analysis came a basic collection of analysis scripts, with are contained in the PostAnalyzer folder. It contains one main executable script. Let us execute it: cd $LEAFPATH/MyAnalysis/Analyzer/PostAnalyzer ./steer.py This will produce two results at the moment. They are new plots to be found in the usual output folder. To get there, do cd $LEAFPATH/../MyAnalysis/2017/Preselection/InitialSetup/plots The first are two plots of the efficiency of each selection step applied in the beginning of this quickstart, one with a linear and one with a logarithmic y-axis scale. This type of plot is useful for monitoring the behavior of the event selections applied. The second new result is the expected upper limit on our test signal cross section, as extracted from the selected background and signal. Note that the data are not considered here for the moment.","title":"Post-analysis"},{"location":"quickstart/#conclusion","text":"In this short example, you have performed a pre-defined event selection, you have visualized the intermediate steps and the final outcome. Last, you have run additional scripts on your fully selected events to compute the selection efficiency of each step and expected upper limits on the signal production cross section. Details on each LEAF package can be found in this documentation as well, see the links in here .","title":"Conclusion"},{"location":"packages/analyzer/","text":"Analyzer Info coming soon.","title":"Analyzer"},{"location":"packages/analyzer/#analyzer","text":"Info coming soon.","title":"Analyzer"},{"location":"packages/generator/","text":"Generator Info coming soon.","title":"Generator"},{"location":"packages/generator/#generator","text":"Info coming soon.","title":"Generator"},{"location":"packages/plotter/","text":"Plotter Info coming soon.","title":"Plotter"},{"location":"packages/plotter/#plotter","text":"Info coming soon.","title":"Plotter"},{"location":"packages/submitter/","text":"Submitter The Submitter package is a helper tool for the Analyzer . It allows for convenient yet efficient submission submission of jobs to the local cluster. Supported clusters These are the current supproted clusters. Links to tutorials and settings are provided. SLURM PSI guide HTCondor @CERN ( guide ) @ULB It takes care of the parallelized (re-)submission of Analyzer jobs, retrieving the output, merging individual and groups of processes and prepare them for plotting. No coding should be necessary for a user of LEAF here. Create user specific settings This allows to store user-specific options to run the jobs into the cluster. The output in the form of a dictionary is store in a json inside the $SUBMITTERPATH folder. An example is given in $SUBMITTERPATH/Settings_exampleuser.json . To create your personal setting, you can simply create your own json (The path and naming scheme MUST be the same. Just place you username instead of exampleuser.), or run the following command. ./createUserSettings.py -u <username> -k <cluster> -e <email> Note This command should be done only once, or anytime one wants to update its own settings. Usage Once you set up your own user settings , you can simply run you xml file as: submit.py MyAnalysis.xml <option-1> <option-2> <arg-option-2> <option-3> Multiple options, available in the Submitter class are allowed: Long name Short name Default Description --clean -c False Remove the local and the remote workdir. --divide -d False Divide the xml file into chunks, create workdir. --local -l Run split jobs locally. --cluster -k Overwrite cluster settings defined by the user specific settings. --submit -s False Submit split jobs to cluster. --output -o False Check the status of the expected output files. --add -a False Add split files back together for samples that are fully processed. --forceadd -f False Force hadding. --notreeadd -t False Add split files without adding the trees. --allowincomplete -i False Ignore the non-existent files and does the hadd nonetheless. --plothadd -p False Hadd files to groups used for plotting and further analysis. Inside-the-box There is some magic going inside the above settings. Here, the minimum info for the expert user. CondorBase This class is designed to make usage of the python bindings for HTCondor . It takes care of creating the job info (default values are stored, but can always be modified via ModifyJobInfo ). The minimal inputs are the executable and its arguments. This class takes care of submitting a single job ( SubmitJob ) as well as multiple jobs ( SubmitManyJobs ). The latter is implemented as default in the Submitter class. Already available, but not implemented anywhere yet, is the CheckStatus option to monitor the job queue. This class is generic and can be easily used to submit any type of job to the HTCondor, as it's not linked to the xml structure provided by the main Analyser .","title":"Submitter"},{"location":"packages/submitter/#submitter","text":"The Submitter package is a helper tool for the Analyzer . It allows for convenient yet efficient submission submission of jobs to the local cluster.","title":"Submitter"},{"location":"packages/submitter/#supported-clusters","text":"These are the current supproted clusters. Links to tutorials and settings are provided. SLURM PSI guide HTCondor @CERN ( guide ) @ULB It takes care of the parallelized (re-)submission of Analyzer jobs, retrieving the output, merging individual and groups of processes and prepare them for plotting. No coding should be necessary for a user of LEAF here.","title":"Supported clusters"},{"location":"packages/submitter/#create-user-specific-settings","text":"This allows to store user-specific options to run the jobs into the cluster. The output in the form of a dictionary is store in a json inside the $SUBMITTERPATH folder. An example is given in $SUBMITTERPATH/Settings_exampleuser.json . To create your personal setting, you can simply create your own json (The path and naming scheme MUST be the same. Just place you username instead of exampleuser.), or run the following command. ./createUserSettings.py -u <username> -k <cluster> -e <email> Note This command should be done only once, or anytime one wants to update its own settings.","title":"Create user specific settings"},{"location":"packages/submitter/#usage","text":"Once you set up your own user settings , you can simply run you xml file as: submit.py MyAnalysis.xml <option-1> <option-2> <arg-option-2> <option-3> Multiple options, available in the Submitter class are allowed: Long name Short name Default Description --clean -c False Remove the local and the remote workdir. --divide -d False Divide the xml file into chunks, create workdir. --local -l Run split jobs locally. --cluster -k Overwrite cluster settings defined by the user specific settings. --submit -s False Submit split jobs to cluster. --output -o False Check the status of the expected output files. --add -a False Add split files back together for samples that are fully processed. --forceadd -f False Force hadding. --notreeadd -t False Add split files without adding the trees. --allowincomplete -i False Ignore the non-existent files and does the hadd nonetheless. --plothadd -p False Hadd files to groups used for plotting and further analysis.","title":"Usage"},{"location":"packages/submitter/#inside-the-box","text":"There is some magic going inside the above settings. Here, the minimum info for the expert user.","title":"Inside-the-box"},{"location":"packages/submitter/#condorbase","text":"This class is designed to make usage of the python bindings for HTCondor . It takes care of creating the job info (default values are stored, but can always be modified via ModifyJobInfo ). The minimal inputs are the executable and its arguments. This class takes care of submitting a single job ( SubmitJob ) as well as multiple jobs ( SubmitManyJobs ). The latter is implemented as default in the Submitter class. Already available, but not implemented anywhere yet, is the CheckStatus option to monitor the job queue. This class is generic and can be easily used to submit any type of job to the HTCondor, as it's not linked to the xml structure provided by the main Analyser .","title":"CondorBase"},{"location":"packages/tuplizer/","text":"Tuplizer Info coming soon.","title":"Tuplizer"},{"location":"packages/tuplizer/#tuplizer","text":"Info coming soon.","title":"Tuplizer"}]}